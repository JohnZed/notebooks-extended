{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting NYC Taxi Fares with RAPIDS\n",
    "\n",
    "[RAPIDS](https://rapids.ai/) is a suite of GPU accelerated data science libraries with APIs that should be familiar to users of Pandas, Dask, and Scikitlearn.\n",
    "\n",
    "Anaconda has graciously made some of the NYC Taxi dataset available in [a public Google Cloud Storage bucket](https://console.cloud.google.com/storage/browser/anaconda-public-data/nyc-taxi/csv/).\n",
    "\n",
    "This notebook builds a simple data pipeline to load the data with cuDF (or Pandas), analyze it with cuML (or scikit-learn), and then try some steps with multiple GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cuml\n",
    "import cudf\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting the Data\n",
    "\n",
    "Let's start with a familiar Pandas approach then port it to RAPIDS in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../../data/nyc-taxi/2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '../../data/nyc-taxi/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Pandas\n",
    "\n",
    "df_2014 = pd.read_csv(base_path+'2014/yellow_tripdata_2014-03.csv')\n",
    "df_2014.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# TODO: Read the CSV with cudf into gdf_2014\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleanup\n",
    "\n",
    "As usual, the data needs to be massaged a bit before we can start adding features that are useful to an ML model.\n",
    "\n",
    "For example, in the 2014 taxi CSV files, there are `pickup_datetime` and `dropoff_datetime` columns.\n",
    "\n",
    "Also, some CSV files have column names with extraneous spaces in them.\n",
    "\n",
    "We'll do a little string manipulation, column renaming, and concatenating of DataFrames to sidestep the problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of column names that need to be re-mapped\n",
    "remap = {}\n",
    "remap['tpep_pickup_datetime'] = 'pickup_datetime'\n",
    "remap['tpep_dropoff_datetime'] = 'dropoff_datetime'\n",
    "remap['ratecodeid'] = 'rate_code'\n",
    "\n",
    "# create a list of columns & dtypes the df must have\n",
    "# note that float64 will be significantly slower on some GPUs (most GeForce, also Tesla T4)\n",
    "must_haves = {\n",
    " 'pickup_datetime': 'datetime64[ms]',\n",
    " 'dropoff_datetime': 'datetime64[ms]',\n",
    " 'passenger_count': 'int32',\n",
    " 'trip_distance': 'float',\n",
    " 'pickup_longitude': 'float',\n",
    " 'pickup_latitude': 'float',\n",
    " 'rate_code': 'int32',\n",
    " 'dropoff_longitude': 'float',\n",
    " 'dropoff_latitude': 'float',\n",
    " 'fare_amount': 'float'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function which takes a DataFrame and fixes column types\n",
    "def clean_columns(df_part, remap, must_haves, float_type):    \n",
    "    # iterate through columns in this df\n",
    "    for col in df_part.columns:\n",
    "        # drop anything not in our expected list\n",
    "        if col not in must_haves:\n",
    "            print(f\"Dropping ({col})\")\n",
    "            df_part = df_part.drop(columns=col)\n",
    "            continue\n",
    "        \n",
    "        if df_part[col].dtype == 'object' and col in ['pickup_datetime', 'dropoff_datetime']:\n",
    "            df_part[col] = df_part[col].astype('datetime64[ms]')\n",
    "            continue\n",
    "                \n",
    "        # if column was read as a string, recast as float\n",
    "        if df_part[col].dtype == 'object':\n",
    "            df_part[col] = df_part[col].str.fillna('-1')\n",
    "            df_part[col] = df_part[col].astype('float32')\n",
    "        else:\n",
    "            # downcast from 64bit to 32bit types\n",
    "            # Tesla T4 are faster on 32bit ops\n",
    "            if 'int' in str(df_part[col].dtype):\n",
    "                df_part[col] = df_part[col].astype('int32')\n",
    "            if 'float' in str(df_part[col].dtype):\n",
    "                # CPU-based pandas has a bug preventing query use with fp32 columns\n",
    "                df_part[col] = df_part[col].astype(float_type)\n",
    "            df_part[col] = df_part[col].fillna(-1)\n",
    "    \n",
    "    return df_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Pandas approach\n",
    "\n",
    "# some col-names include pre-pended spaces remove & lowercase column names\n",
    "col_cleanup = {col: col.strip().lower() for col in list(df_2014.columns)}\n",
    "df = df_2014.rename(columns=col_cleanup)\n",
    "# rename columns using the supplied mapping\n",
    "df = df.rename(remap)\n",
    "\n",
    "df = clean_columns(df, remap, must_haves, np.float64)\n",
    "print(df.__class__)\n",
    "print(df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# TODO: RAPIDS approach - same as Pandas, but generate 'gdf' as output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at some key stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=\"passenger_count\", y=\"fare_amount\", data=gdf.to_pandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x=\"pickup_longitude\", y=\"pickup_latitude\",\n",
    "           data=gdf.head(100000).to_pandas(),\n",
    "           fit_reg=False,\n",
    "           x_jitter=0.01, y_jitter=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# apply a list of filter conditions to throw out records with missing or outlier values\n",
    "query_frags = [\n",
    "    'fare_amount > 0 and fare_amount < 500',\n",
    "    'passenger_count > 0 and passenger_count < 6',\n",
    "    'pickup_longitude > -75.0 and pickup_longitude < -73.0',\n",
    "    'dropoff_longitude > -75 and dropoff_longitude < -73',\n",
    "    'pickup_latitude > 40 and pickup_latitude < 42',\n",
    "    'dropoff_latitude > 40 and dropoff_latitude < 42'\n",
    "]\n",
    "df_subset = df.query(' and '.join(query_frags)).copy()\n",
    "\n",
    "# inspect the results of cleaning\n",
    "df_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# TODO: RAPIDS version with \"gdf_subset\" as output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo 3: UDFs to add rich features \n",
    "\n",
    "cuDF provides standard DataFrame operations, but also let you run \"user defined functions\" on the underlying data.\n",
    "\n",
    "cuDF's [apply_rows](https://rapidsai.github.io/projects/cudf/en/0.6.0/api.html#cudf.dataframe.DataFrame.apply_rows) operation is similar to Pandas's [DataFrame.apply](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.apply.html), except that for cuDF, custom Python code is [JIT compiled by numba](https://numba.pydata.org/numba-doc/dev/cuda/kernels.html) into GPU kernels.\n",
    "\n",
    "We'll use a Haversine Distance calculation to find total trip distance, and extract additional useful variables from the datetime fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import pi\n",
    "\n",
    "def haversine_distance_kernel_cpu(row):\n",
    "    x_1, y_1, x_2, y_2 = (row[\"pickup_latitude\"], row[\"pickup_longitude\"], row[\"dropoff_latitude\"], row[\"dropoff_longitude\"])\n",
    "    x_1 = pi/180 * x_1\n",
    "    y_1 = pi/180 * y_1\n",
    "    x_2 = pi/180 * x_2\n",
    "    y_2 = pi/180 * y_2\n",
    "\n",
    "    dlon = y_2 - y_1\n",
    "    dlat = x_2 - x_1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(x_1) * np.cos(x_2) * np.sin(dlon/2)**2\n",
    "\n",
    "    c = 2 * np.arcsin(np.sqrt(a)) \n",
    "    r = 6371 # Radius of earth in kilometers\n",
    "        \n",
    "    return c * r\n",
    "        \n",
    "def add_features(df):\n",
    "    df['hour'] = df['pickup_datetime'].dt.hour\n",
    "    df['year'] = df['pickup_datetime'].dt.year\n",
    "    df['month'] = df['pickup_datetime'].dt.month\n",
    "    df['day'] = df['pickup_datetime'].dt.day\n",
    "    \n",
    "    df['pickup_latitude_r'] = df['pickup_latitude']//.01*.01\n",
    "    df['pickup_longitude_r'] = df['pickup_longitude']//.01*.01\n",
    "    df['dropoff_latitude_r'] = df['dropoff_latitude']//.01*.01\n",
    "    df['dropoff_longitude_r'] = df['dropoff_longitude']//.01*.01\n",
    "    df[\"day_of_week\"] = df[\"pickup_datetime\"].dt.dayofweek\n",
    "    \n",
    "    df = df.drop(columns=['pickup_datetime', 'dropoff_datetime'])    \n",
    "    \n",
    "    df['is_weekend'] = (df['day_of_week'] >= 5).astype(np.int32)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# actually add the features\n",
    "taxi_df = add_features(df_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# compute distance\n",
    "taxi_df[\"h_distance\"] = haversine_distance_kernel_cpu(taxi_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cuDF version with UDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cuDF's [apply_rows](https://rapidsai.github.io/projects/cudf/en/0.6.0/api.html#cudf.dataframe.DataFrame.apply_rows) operation is similar to Pandas's [DataFrame.apply](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.apply.html), except that for cuDF, custom Python code is [JIT compiled by numba](https://numba.pydata.org/numba-doc/dev/cuda/kernels.html) into GPU kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from math import cos, sin, asin, sqrt, pi\n",
    "\n",
    "def haversine_distance_kernel_gpu(pickup_latitude, pickup_longitude, dropoff_latitude, dropoff_longitude, h_distance):\n",
    "    for i, (x_1, y_1, x_2, y_2) in enumerate(zip(pickup_latitude, pickup_longitude, dropoff_latitude, dropoff_longitude)):\n",
    "        x_1 = pi/180 * x_1\n",
    "        y_1 = pi/180 * y_1\n",
    "        x_2 = pi/180 * x_2\n",
    "        y_2 = pi/180 * y_2\n",
    "        \n",
    "        dlon = y_2 - y_1\n",
    "        dlat = x_2 - x_1\n",
    "        a = sin(dlat/2)**2 + cos(x_1) * cos(x_2) * sin(dlon/2)**2\n",
    "        \n",
    "        c = 2 * asin(sqrt(a)) \n",
    "        r = 6371 # Radius of earth in kilometers\n",
    "        \n",
    "        h_distance[i] = c * r\n",
    "\n",
    "def day_of_the_week_kernel(day, month, year, day_of_week):\n",
    "    for i, (d_1, m_1, y_1) in enumerate(zip(day, month, year)):\n",
    "        if month[i] <3:\n",
    "            shift = month[i]\n",
    "        else:\n",
    "            shift = 0\n",
    "        Y = year[i] - (month[i] < 3)\n",
    "        y = Y - 2000\n",
    "        c = 20\n",
    "        d = day[i]\n",
    "        m = month[i] + shift + 1\n",
    "        day_of_week[i] = (d + math.floor(m*2.6) + y + (y//4) + (c//4) -2*c)%7\n",
    "        \n",
    "def add_features_gpu(df):\n",
    "    df['hour'] = df['pickup_datetime'].dt.hour\n",
    "    df['year'] = df['pickup_datetime'].dt.year\n",
    "    df['month'] = df['pickup_datetime'].dt.month\n",
    "    df['day'] = df['pickup_datetime'].dt.day\n",
    "    \n",
    "    df['pickup_latitude_r'] = df['pickup_latitude']//.01*.01\n",
    "    df['pickup_longitude_r'] = df['pickup_longitude']//.01*.01\n",
    "    df['dropoff_latitude_r'] = df['dropoff_latitude']//.01*.01\n",
    "    df['dropoff_longitude_r'] = df['dropoff_longitude']//.01*.01\n",
    "    # df[\"day_of_week\"] = df[\"pickup_datetime\"].dt.dayofweek\n",
    "    # cuDF does not support dayofweek yet, coming soon though: https://github.com/rapidsai/cudf/pull/2814\n",
    "    \n",
    "    #\n",
    "    # Auto-JIT kernels\n",
    "    #\n",
    "    df = df.apply_rows(day_of_the_week_kernel,\n",
    "                      incols=['day', 'month', 'year'],\n",
    "                      outcols=dict(day_of_week=np.float32),\n",
    "                      kwargs=dict())\n",
    "    df = df.drop(columns=['pickup_datetime', 'dropoff_datetime'])    \n",
    "\n",
    " \n",
    "    df['is_weekend'] = (df['day_of_week'] >= 5).astype(np.int32)\n",
    "    return df\n",
    "\n",
    "def compute_distance_gpu(df):\n",
    "    df = df.apply_rows(haversine_distance_kernel_gpu,\n",
    "                   incols=['pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude'],\n",
    "                   outcols=dict(h_distance=np.float32),\n",
    "                   kwargs=dict())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# TODO: actually add the features and create \"taxi_gdf\" from gdf_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# TODO: add the distance calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more advanced spatial calculations, check out cuSpatial (https://medium.com/rapids-ai/releasing-cuspatial-to-accelerate-geospatial-and-spatiotemporal-processing-b686d8b32a9), the newest RAPIDS library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Print summary stats from \"taxi_gdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pick a Training Set\n",
    "\n",
    "Let's imagine you're making a trip to New York on the 24th and want to build a model to predict what fare prices will be like the last few days of the month based on the first part of the month. We'll use a query expression to identify the `day` of the month to use to divide the data into train and test sets.\n",
    "\n",
    "The wall-time below represents how long it takes your GPU cluster to load data from the Google Cloud Storage bucket and the ETL portion of the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_train_df = taxi_df.query('day < 24')\n",
    "Y_train_df = X_train_df[['fare_amount']]\n",
    "X_train_df = X_train_df.drop(columns='fare_amount')\n",
    "\n",
    "# numpy versions for sklearn\n",
    "X_train_np = X_train_df.to_numpy()\n",
    "Y_train_np = Y_train_df.to_numpy()\n",
    "len(X_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.utils import input_utils # Helpful functions for managing gpu arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_train_gdf = taxi_gdf.query('day < 24')\n",
    "Y_train_gdf = X_train_gdf[['fare_amount']]\n",
    "X_train_gdf = X_train_gdf.drop(columns='fare_amount')\n",
    "\n",
    "# gpu matrix versions for cuml\n",
    "X_train_gpu = input_utils.input_to_dev_array(X_train_gdf, convert_to_dtype=np.float32).array\n",
    "Y_train_gpu = input_utils.input_to_dev_array(Y_train_gdf, convert_to_dtype=np.float32).array\n",
    "\n",
    "# test versions\n",
    "X_test_gdf = taxi_gdf.query('day >= 24')\n",
    "Y_test_gdf = X_test_gdf[['fare_amount']]\n",
    "X_test_gdf = X_test_gdf.drop(columns='fare_amount')\n",
    "\n",
    "X_test_gpu = input_utils.input_to_dev_array(X_test_gdf, convert_to_dtype=np.float32).array\n",
    "Y_test_gpu = input_utils.input_to_dev_array(Y_test_gdf, convert_to_dtype=np.float32).array\n",
    "\n",
    "\n",
    "len(X_train_gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo 4: Cluster and analyze with cuML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn, sklearn.cluster\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# use scikit-learn on CPU\n",
    "\n",
    "sk_kmeans = sklearn.cluster.KMeans(n_clusters=5, n_jobs=-1)\n",
    "train_clusters_cpu = sk_kmeans.fit_predict(X_train_np[:200000,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n_samples = 400000\n",
    "\n",
    "# TODO: use cuML on GPU to fit KMeans with 5 clusters (larger dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just take a subset to speed plotting\n",
    "gdf_train_head = X_train_gdf.iloc[:400000]\n",
    "gdf_train_head[\"cluster\"] = train_clusters_gpu[:400000]\n",
    "gdf_train_head[\"short_trip\"] = gdf_train_head[\"trip_distance\"] < 1.01 # About the 25th percentile\n",
    "gdf_train_head[\"is_rush_est\"] = ((gdf_train_head.hour >= 10) & (gdf_train_head.hour <= 14)) | \\\n",
    "                                ((gdf_train_head.hour >= 21) & (gdf_train_head.hour <= 24))\n",
    "\n",
    "# actually do the plot\n",
    "sns.lmplot(\"pickup_longitude\", \"pickup_latitude\", data=gdf_train_head.to_pandas(),\n",
    "           hue=\"cluster\", col=\"is_rush_est\", row=\"short_trip\", fit_reg=False, scatter_kws={\"s\": 10})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a simple supervised model with cuML\n",
    "\n",
    "cuML supports a large range of supervised models, all emulating the scikit-learn interfaces. See the README (https://github.com/rapidsai/cuml) for a recent list. Here, we'll try a very simple model - a regularized linear regression with the ElasticNet approach that blends L1 and L2 penalties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet as skElastic\n",
    "from cuml.linear_model import ElasticNet as cuElastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Sklearn will parallelize over all CPU cores with n_jobs=-1\n",
    "sk_model = skElastic(alpha=0.1)\n",
    "sk_model.fit(X_train_np, Y_train_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# TODO: Build a similar model on GPU with cuML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Predict on a test set (storing as \"enet_predictions\" in the Y_test_gdf df)\n",
    "# and evaluate the predictions' R2 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo 5: Train an  XGBoost Regression Model\n",
    "\n",
    "XGBoost is one of the most popular packages for gradient boosted decision trees. It comes with excellent GPU acceleration out of the box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Train on CPU (uses all CPUs by default)\n",
    "import xgboost\n",
    "\n",
    "params = {\n",
    " 'learning_rate': 0.3,\n",
    "  'max_depth': 6,\n",
    "\n",
    "  'subsample': 0.6,\n",
    "  'gamma': 1\n",
    "}\n",
    "\n",
    "train_dmat = xgboost.DMatrix(X_train_np, Y_train_np, feature_names=X_train_df.columns)\n",
    "print(\"Converted to dmatrix\")\n",
    "trained_model = xgboost.train(params, train_dmat, num_boost_round=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# TODO: retrain on GPU for more rounds, saving model as trained_model_gpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Good is Our Model?\n",
    "\n",
    "Now that we have a trained model, we need to test it with the ecords we held out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# TODO: generate predictions on the test set as Y_test_gdf['prediction']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Root Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_gdf[\"squared_error\"] = (Y_test_gdf['prediction'] - Y_test_gdf['fare_amount'])**2\n",
    "\n",
    "# inspect the results to make sure our calculation looks right\n",
    "Y_test_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the actual RMSE over the full test set\n",
    "np.sqrt(Y_test_gdf.squared_error.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Trained Model for Later Use\n",
    "\n",
    "To make a model maximally useful, you need to be able to save it for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "trained_model_gpu.save_model(\"output.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo 6: A quick intro to Dask + RAPIDS\n",
    "\n",
    "Dask is a sophisticated package for parallel computation with a number of different datatypes. For much more detail, see: https://tutorial.dask.org/\n",
    "\n",
    "In these examples, we'll focus on the basics of `dask_cudf` and `dask_cuda`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask_cudf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask, dask_cudf\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client, wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup a cluster and connect a client to it\n",
    "\n",
    "cluster = LocalCUDACluster()\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ddf_all2014 = dask_cudf.read_csv(base_path+'2014/yellow_*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ddf_all2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redo the same cleanup we used above but on Dask this time\n",
    "\n",
    "col_cleanup = {col: col.strip().lower() for col in list(df_2014.columns)}\n",
    "ddf = ddf_all2014.rename(columns=col_cleanup)\n",
    "# rename columns using the supplied mapping\n",
    "ddf = ddf.rename(columns=remap)\n",
    "\n",
    "ddf = clean_columns(ddf, remap, must_haves, np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute a simple histogram of passengers\n",
    "\n",
    "value_counts = ddf.passenger_count.value_counts()\n",
    "print(value_counts)\n",
    "print(value_counts.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning with Dask\n",
    "\n",
    "See also XGBoost's Dask interface docs: https://github.com/dmlc/xgboost/tree/master/demo/dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use map_partitions to apply the same distance function we used before\n",
    "ddf_with_dist = ddf.map_partitions(compute_distance_gpu)\n",
    "ddf_with_dist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_cols = [\"passenger_count\", \"trip_distance\", \"rate_code\", \"fare_amount\", \"h_distance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_ddf = ddf_with_dist[kmeans_cols]\n",
    "for c in X_ddf.columns:\n",
    "    X_ddf[c] = X_ddf[c].astype(np.float32)\n",
    "Y_ddf = X_ddf[\"fare_amount\"]\n",
    "X_ddf = X_ddf.drop(columns=\"fare_amount\")\n",
    "\n",
    "X_ddf, y_ddf = client.persist([X_ddf, Y_ddf]) # Trigger the computation and cache in RAM\n",
    "_ = wait([X_ddf, y_ddf]) # Actually wait for persistence to finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "from xgboost.dask import DaskDMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "params = {'verbosity': 2, 'nthread': 1, 'tree_method': 'gpu_hist', 'objective': 'reg:squarederror',}\n",
    "\n",
    "# TODO: create a dmatrix, train a model, and store as xgb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: explore within-train-sample predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
